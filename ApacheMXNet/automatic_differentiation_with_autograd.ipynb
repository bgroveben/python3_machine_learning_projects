{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Automatic differentiation with `autograd`](http://gluon.mxnet.io/chapter01_crashcourse/autograd.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, we *train* models to get better and better as a function of experience.  \n",
    "Usually, *getting better* means minimizing a *loss function*, i.e. a score that answers -- how *bad* is our model?  \n",
    "With neural networks, we choose loss functions to be differentiable with respect to our parameters.  \n",
    "Put simply, this means that for each of the model's parameters, we can determine how much *increasing* or *decreasing* it might affect the loss.  \n",
    "While the calculations are straightforward, for complex models, working it out by hand can be a pain.  \n",
    "_MXNet_'s `autograd` package expedites this work by automatically calculating derivatives.  \n",
    "And while most other libraries require that we compile a symbolic graph to take automatic derivatives, ``mxnet.autograd``, like PyTorch, allows you to take derivatives while writing  ordinary imperative code.  \n",
    "Every time you make a pass through your model, ``autograd`` builds a graph on the fly, through which it can immediately backpropagate gradients.  \n",
    "Let's go through it step by step.  \n",
    "For this tutorial, we'll only need to import ``mxnet.ndarray``, and ``mxnet.autograd``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attaching gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a toy example, let's say that we are interested in differentiating a function ``f = 2 * (x ** 2)`` with respect to parameter x.  \n",
    "We can start by assigning an initial value of ``x``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 2.]\n",
       " [3. 4.]]\n",
       "<NDArray 2x2 @cpu(0)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nd.array([[1, 2], [3, 4]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we compute the gradient of `f` with respect to `x`, we'll need a place to store it.  \n",
    "In `MXNet`, we can tell an `NDArray` that we plan to store a gradient by invoking its `attach_grad()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns None type:\n",
    "x.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’re going to define the function `f` and MXNet will generate a computation graph on the fly.  \n",
    "It’s as if MXNet turned on a recording device and captured the exact path by which each variable was generated.  \n",
    "Note that building the computation graph requires a nontrivial amount of computation.  \n",
    "MXNet will only build the graph when explicitly told to do so.  \n",
    "We can instruct MXNet to start recording by placing code inside a `with autograd.record():` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "<NDArray 2x2 @cpu(0)>\n",
      "\n",
      "[[2. 4.]\n",
      " [6. 8.]]\n",
      "<NDArray 2x2 @cpu(0)>\n",
      "\n",
      "[[ 2.  8.]\n",
      " [18. 32.]]\n",
      "<NDArray 2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x * 2\n",
    "    z = y * x\n",
    "    \n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation time.  \n",
    "When `z` has more than one entry, `z.backward()` is equivalent to `mx.nd.sum(z).backward()`.  \n",
    "Got that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns None type\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's determine the expected output.  \n",
    "Remember that `y = x * 2`, and `z = x * y`, so `z` should be equal to `2 * x * x`.  \n",
    "After doing backpropagation with `z.backward()`, we expect to get back gradient `dz/dx` as follows:  \n",
    "`dy/dx = 2`  \n",
    "`dz/dx = 4 * x`  \n",
    "If everything goes according to plan, `x.grad` should consist of an NDArray with the values `[[4, 8], [12, 16]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 4.  8.]\n",
      " [12. 16.]]\n",
      "<NDArray 2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Head gradients and the chain rule](http://gluon.mxnet.io/chapter01_crashcourse/autograd.html#Head-gradients-and-the-chain-rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes when we call the ``backward`` method on an NDArray, e.g. ``y.backward()``, where ``y`` is a function of ``x`` we are just interested in the derivative of ``y`` with respect to ``x``.  \n",
    "Mathematicians write this as $\\frac{dy(x)}{dx}$.  \n",
    "At other times, we may be interested in the gradient of ``z`` with respect to ``x``, where ``z`` is a function of ``y``, which in turn, is a function of ``x``.  \n",
    "That is, we are interested in $\\frac{d}{dx} z(y(x))$.  \n",
    "Knowing how to differentiate composite functions will come in handy here.  \n",
    "Recall that by the chain rule $\\frac{d}{dx} z(y(x)) = \\frac{dz(y)}{dy} \\frac{dy(x)}{dx}$.  \n",
    "So, when ``y`` is part of a larger function ``z``, and we want ``x.grad`` to store $\\frac{dz}{dx}$, we can pass in the *head gradient* $\\frac{dz}{dy}$ as an input to ``backward()``.  \n",
    "The default argument is ``nd.ones_like(y)``.  \n",
    "See [Wikipedia](https://en.wikipedia.org/wiki/Chain_rule) and [Khan Academy](https://www.khanacademy.org/math/ap-calculus-ab/ab-derivative-rules/ab-chain-rule/a/chain-rule-review) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[40.    8.  ]\n",
      " [ 1.2   0.16]]\n",
      "<NDArray 2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x * 2\n",
    "    z = y * x\n",
    "    \n",
    "head_gradient = nd.array([[10, 1.], [.1, .01]])\n",
    "z.backward(head_gradient)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the basics, we can do some wild things with autograd, including building differentiable functions using Pythonic control flow.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nd.random_normal(shape=3)\n",
    "a.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "    b = a * 2\n",
    "    while (nd.norm(b) < 1000).asscalar():\n",
    "        b = b * 2\n",
    "        \n",
    "    if (mx.nd.sum(b) > 0).asscalar():\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_gradient = nd.array([0.01, 0.1, 1.0])\n",
    "c.backward(head_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[  1024.  10240. 102400.]\n",
      "<NDArray 3 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
