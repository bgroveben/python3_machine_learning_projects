{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Getting Started with the Gluon Interface](https://github.com/gluon-api/gluon-api#getting-started-with-the-gluon-interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example from the Github repository we will build and train a simple two-layer artificial neural network (ANN) called a multilayer perceptron.  \n",
    "First, we need to import `mxnet` and MXNet's implementation of the `gluon` specification.  \n",
    "We will also need `autograd`, `ndarray`, and `numpy`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd, ndarray\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use `gluon.data.DataLoader`, Gluon's data iterator, to hold the training and test data.  \n",
    "Iterators are a useful object class for traversing through large datasets.  \n",
    "We pass Gluon's `DataLoader` a helper, `gluon.data.vision.MNIST`, that will pre-process the MNIST handwriting dataset, getting into the right size and format, using parameters to tell it which is test set and which is the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=lambda data, label: \n",
    "                                     (data.astype(np.float32)/255, label)), batch_size=32, shuffle=True)\n",
    "\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=lambda data, label:\n",
    "                                    (data.astype(np.float32)/255, label)),batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to define the actual neural network, and we can do so in five simple lines of code.  \n",
    "First, we initialize the network with `net = gluon.nn.Sequential()`.  \n",
    "Then, with that `net`, we create three layers using `gluon.nn.Dense`:  \n",
    "The first will have 128 nodes, and the second will have 64 nodes.  \n",
    "They both incorporate the `relu` by passing that into the `activation` function parameter.  \n",
    "The final layer for our model, `gluon.nn.Dense(10)`, is used to set up the output layer with the number of nodes corresponding to the total number of possible outputs.  \n",
    "In our case with MNIST, there are only 10 possible outputs because the pictures represent numerical digits of which there are only 10 (i.e., 0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model:\n",
    "net = gluon.nn.Sequential()\n",
    "# Define the model architecture:\n",
    "with net.name_scope():\n",
    "    # The first layer has 128 nodes:\n",
    "    net.add(gluon.nn.Dense(128, activation=\"relu\"))\n",
    "    # The second layer has 64 nodes:\n",
    "    net.add(gluon.nn.Dense(64, activation=\"relu\"))\n",
    "    # The output layer has 10 possible outputs:\n",
    "    net.add(gluon.nn.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to kicking off the model training process, we need to initialize the model’s parameters and set up the loss with `gluon.loss.SoftmaxCrossEntropyLoss()` and model optimizer functions with `gluon.Trainer`.  \n",
    "As with creating the model, these normally complicated functions are distilled to one line of code each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin with pseudorandom values for all of the model's parameters from a normal distribution\n",
    "# with a standard deviation of 0.05:\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=0.05))\n",
    "\n",
    "# Use the softmax cross entropy loss function to measure how well the model is able to predict\n",
    "# the correct answer:\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "# Use stochastic gradient descent to train the model and set the learning rate hyperparameter to .1:\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the training is fairly typical and all the while using Gluon's functionality to make the process simple and seamless.  \n",
    "There are four steps:  \n",
    "1) pass in a batch of data;  \n",
    "2) calculate the difference between the output generated by the neural network model and the actual truth (i.e., the loss);  \n",
    "3) use Gluon's `autograd` to calculate the derivatives of the model’s parameters with respect to their impact on the loss;  \n",
    "4) use Gluon's `trainer` method to optimize the parameters in a way that will decrease the loss.  \n",
    "We set the number of epochs at 10, meaning that we will cycle through the entire training dataset 10 times.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Current Loss: 0.1128312274813652.\n",
      "Epoch 1. Current Loss: 0.14288707077503204.\n",
      "Epoch 2. Current Loss: 0.020673630759119987.\n",
      "Epoch 3. Current Loss: 0.12083777785301208.\n",
      "Epoch 4. Current Loss: 0.013107175938785076.\n",
      "Epoch 5. Current Loss: 0.04734328016638756.\n",
      "Epoch 6. Current Loss: 0.11852305382490158.\n",
      "Epoch 7. Current Loss: 0.007883351296186447.\n",
      "Epoch 8. Current Loss: 0.04016139730811119.\n",
      "Epoch 9. Current Loss: 0.002568621188402176.\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(mx.cpu()).reshape((-1, 784))\n",
    "        label = label.as_in_context(mx.cpu())\n",
    "        # Start calculating and recording the derivatives:\n",
    "        with autograd.record():\n",
    "            # Optimize parameters -- Forward iteration:\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        # Record statistics on the model's performance over each epoch:\n",
    "        curr_loss = ndarray.mean(loss).asscalar()\n",
    "    print(\"Epoch {}. Current Loss: {}.\".format(e, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a trained neural network model, and can see how the accuracy improves over each epoch.  \n",
    "A [Jupyter notebook of this code](https://github.com/gluon-api/gluon-api/blob/master/tutorials/mnist-gluon-example.ipynb) has been provided for your convenience.\n",
    "\n",
    "To learn more about the Gluon interface and deep learning, you can reference this [comprehensive set of tutorials](http://gluon.mxnet.io/), which covers everything from an introduction to deep learning to how to implement cutting-edge neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
