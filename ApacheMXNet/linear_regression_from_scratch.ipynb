{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Linear regression from scratch](http://gluon.mxnet.io/chapter02_supervised-learning/linear-regression-scratch.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powerful ML libraries can eliminate repetitive work, but if you rely too much on abstractions, you might never learn how neural networks really work under the hood.  \n",
    "So for this first example, let’s get our hands dirty and build everything from scratch, relying only on a`utograd` and `NDArray`.  \n",
    "First, we’ll import the same dependencies as in the [autograd chapter](http://gluon.mxnet.io/chapter01_crashcourse/autograd.html).  \n",
    "We’ll also import the powerful `gluon` package but in this chapter, we’ll only be using it for data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The kernel for this notebook is running Python 3, but we'll see:\n",
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll also want to specify the contexts where computation should happen.  \n",
    "This tutorial is so simple that you could probably run it on a calculator watch.  \n",
    "But, to develop good habits we’re going to specify two contexts: one for data and one for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get our feet wet, we'll start off by looking at the problem of regression.  \n",
    "This is the task of predicting a *real valued target* $y$ given a data point $x$.  \n",
    "In linear regression, the simplest and still perhaps the most useful approach, we assume that prediction can be expressed as a *linear* combination of the input features (thus giving the name *linear* regression):  \n",
    "$$\\hat{y} = w_1 \\cdot x_1 + ... + w_d \\cdot x_d + b$$  \n",
    "Given a collection of data points $X$, and corresponding target values $\\boldsymbol{y}$ we'll try to find the *weight* vector $\\boldsymbol{w}$ and bias term $b$ (also called an *offset* or *intercept*) that approximately associate data points $\\boldsymbol{x}_i$ with their corresponding labels ``y_i``.  \n",
    "Using slightly more advanced math notation, we can express the predictions $\\boldsymbol{\\hat{y}}$ corresponding to a collection of datapoints $X$ via the matrix-vector product:  \n",
    "$$\\boldsymbol{\\hat{y}} = X \\boldsymbol{w} + b$$  \n",
    "Before we can get going, we will need two more things:  \n",
    "* Some way to measure the quality of the current model  \n",
    "* Some way to manipulate the model to improve its quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Square loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to say whether we’ve done a good job, we need some way to measure the quality of a model.  \n",
    "Generally, we will define a loss function that says how far are our predictions from the correct answers.  \n",
    "For the classical case of linear regression, we usually focus on the squared error.  \n",
    "Specifically, our loss will be the sum, over all examples, of the squared error $(y_i−\\hat{y})^2)$ on each:  \n",
    "$$\\ell(y, \\hat{y}) = \\sum_{i=1}^n (\\hat{y}_i-y_i)^2.$$  \n",
    "For one-dimensional data, we can easily visualize the relationship between our single feature and the target variable. It’s also easy to visualize a linear predictor and it’s error on each example.  \n",
    "Note that squared loss heavily penalizes outliers.  \n",
    "For the visualized predictor below, the lone outlier would contribute most of the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linear_regression](img/linear_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For us to minimize the error, we need some mechanism to alter the model.  \n",
    "We do this by choosing values of the *parameters* $\\boldsymbol{w}$ and $b$.  \n",
    "This is the only job of the learning algorithm.  \n",
    "Take training data ($X$, $y$) and the functional form of the model $\\hat{y} = X\\boldsymbol{w} + b$.  \n",
    "Learning then consists of choosing the best possible $\\boldsymbol{w}$ and $b$ based on the available evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matters of provenance aside, you might wonder - if Legendre and Gauss worked on linear regression, does that mean there were the original deep learning researchers?  \n",
    "And if linear regression doesn't wholly belong to deep learning, then why are we presenting a linear model as the first example in a tutorial series on neural networks?  \n",
    "Well it turns out that we can express linear regression as the simplest possible (useful) neural network.  \n",
    "A neural network is just a collection of nodes (aka neurons) connected by directed edges.  \n",
    "In most networks, we arrange the nodes into layers with each feeding its output into the layer above.  \n",
    "To calculate the value of any node, we first perform a weighted sum of the inputs (according to weights ``w``) and then apply an *activation function*.  \n",
    "For linear regression, we only have two layers, one corresponding to the input (depicted in orange) and a one-node layer (depicted in green) correspnding to the ouput.  \n",
    "For the output node the activation function is just the identity function.  \n",
    "![](img/onelayer.png)  \n",
    "While you certainly don't have to view linear regression through the lens of deep learning, you can (and we will!).  \n",
    "To ground the concepts that we just discussed in code, let's actually code up a neural network for linear regression from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
