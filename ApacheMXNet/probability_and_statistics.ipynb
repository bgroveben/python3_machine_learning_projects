{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some form or another, machine learning is all about making predictions.  \n",
    "We might want to predict the probability of a patient suffering a heart attack in the next year, given their clinical history.  \n",
    "In anomaly detection, we might want to assess how likely a set of readings from an airplane’s jet engine would be, were it operating normally.  \n",
    "In reinforcement learning, we want an agent to act intelligently in an environment.  \n",
    "This means we need to think about the probability of getting a high reward under each of the available action.  \n",
    "And when we build recommender systems we also need to think about probability.  \n",
    "For example, if we hypothetically worked for a large online bookseller, we might want to estimate the probability that a particular user would buy a particular book, if prompted.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we need to use the language of probability and statistics.  \n",
    "Entire courses, majors, theses, careers, and even departments, are devoted to probability.  \n",
    "So our goal here isn’t to teach the whole subject.  \n",
    "Instead we hope to get you off the ground, to teach you just enough that you know everything necessary to start building your first machine learning models and to have enough of a flavor for the subject that you can begin to explore it on your own if you wish.  \n",
    "We’ve talked a lot about probabilities so far without articulating what precisely they are or giving a concrete example.  \n",
    "Let’s get more serious by considering the problem of distinguishing cats and dogs based on photographs.  \n",
    "This might sound simpler but it’s actually a formidable challenge.  \n",
    "To start with, the difficulty of the problem may depend on the resolution of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/cats_and_dogs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it’s easy for humans to recognize cats and dogs at 320 pixel resolution, it becomes challenging at 40 pixels and next to impossible at 20 pixels.  \n",
    "In other words, our ability to tell cats and dogs apart at a large distance (and thus low resolution) might approach uninformed guessing.  \n",
    "Probability gives us a formal way of reasoning about our level of certainty.  \n",
    "If we are completely sure that the image depicts a cat, we say that the probability that the corresponding label l is cat, denoted $P(l=\\mathrm{cat})$ equals 1.0.  \n",
    "If we had no evidence to suggest that $l=\\mathrm{cat}$ or that $l=\\mathrm{dog}$, then we might say that the two possibilities were equally likely expressing this as $P(l=\\mathrm{cat})=0.5$.  \n",
    "If we were reasonably confident, but not sure that the image depicted a cat, we might assign a probability $.5<P(l=\\mathrm{cat})<1.0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider a second case: given some weather monitoring data, we want to predict the probability that it will rain in Taipei tomorrow.  \n",
    "If it’s summertime, the rain might come with probability $.5$.  \n",
    "In both cases, we have some value of interest.  \n",
    "And in both cases we are uncertain about the outcome.  \n",
    "But there’s a key difference between the two cases.  \n",
    "In this first case, the image is in fact either a dog or a cat, we just don’t know which.  \n",
    "In the second case, the outcome may actually be a random event, if you believe in such things (and most physicists do).  \n",
    "So probability is a flexible language for reasoning about our level of certainty, and it can be applied effectively in a broad set of contexts.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic probability theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say that we cast a die and want to know what the chance is of seeing a $1$ rather than another digit.  \n",
    "If the die is fair, all six outcomes $\\mathcal{X} = \\{1, \\ldots, 6\\}$ are equally likely to occur, hence we would see a $1$ in $1$ out of $6$ cases.  \n",
    "Formally we state that 1 occurs with probability $\\frac{1}{6}$.  \n",
    "For a real die that we receive from a factory, we might not know those proportions and we would need to check whether it is tainted.  \n",
    "The only way to investigate the die is by casting it many times and recording the outcomes.  \n",
    "For each cast of the die, we’ll observe a value $\\{1, 2, \\ldots, 6\\}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these outcomes, we want to investigate the probability of observing each outcome.  \n",
    "One natural approach for each value is to take the individual count for that value and to divide it by the total number of tosses.  \n",
    "This gives us an estimate of the probability of a given event.  \n",
    "The law of large numbers tell us that as the number of tosses grows this estimate will draw closer and closer to the true underlying probability.  \n",
    "Before going into the details of what’s going here, let’s try it out.  \n",
    "We can start by importing the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll want to be able to cast the die.  \n",
    "In statistics we call this process of drawing examples from probability distributions *sampling*.  \n",
    "The distribution which assigns probabilities to a number of discrete choices is called the *multinomial distribution*.  \n",
    "We’ll give a more formal definition of *distribution* later, but at a high level, think of it as just an assignment of probabilities to events.  \n",
    "In MXNet, we can sample from the multinomial distribution via the aptly named `nd.sample_multinomial` function.  \n",
    "The function can be called in many ways, but we’ll focus on the simplest.  \n",
    "To draw a single sample, we simply pass in a vector of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 0.16666667  0.16666667  0.16666667  0.16666667  0.16666667  0.16666667]\n",
      "<NDArray 6 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[2]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = nd.ones(6) / 6\n",
    "print(probabilities)\n",
    "nd.sample_multinomial(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
