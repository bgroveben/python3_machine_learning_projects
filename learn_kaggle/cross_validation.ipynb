{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Cross-Validation](https://www.kaggle.com/dansbecker/cross-validation/code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is an iterative process.  \n",
    "You will face choices about predictive variables to use, what types of models to use, what arguments to supply those models, and many other options.  \n",
    "We make these choices in a data-driven way by measuring model quality of various alternatives.  \n",
    "You've already learned to use `train_test_split` to split the data, so you can measure model quality on the test data.  \n",
    "Cross-validation extends this approach to model scoring (or \"model validation.\")  \n",
    "Compared to `train_test_split`, cross-validation gives you a more reliable measure of your model's quality, though it takes longer to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The shortcomings of train-test split:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you have a dataset with 5000 rows.  \n",
    "The `train_test_split` function has an argument for `test_size` that you can use to decide how many rows go to the training set and how many go to the test set.   \n",
    "The larger the test set, the more reliable your measures of model quality will be.  \n",
    "At an extreme, you could imagine having only 1 row of data in the test set.  \n",
    "If you compare alternative models, which one makes the best predictions on a single data point will be mostly a matter of luck.  \n",
    "You will typically keep about 20% as a test dataset.  \n",
    "But even with 1000 rows in the test set, there's some random chance in determining model scores.  \n",
    "A model might do well on one set of 1000 rows, even if it would be inaccurate on a different 1000 rows.  \n",
    "The larger the test set, the less randomness (aka \"noise\") there is in our measure of model quality.  \n",
    "But we can only get a large test set by removing data from our training data, and smaller training datasets mean worse models.  \n",
    "In fact, the ideal modeling decisions on small datasets typically aren't the best modeling decisions on large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cross-Validation Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
