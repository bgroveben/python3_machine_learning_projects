{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Dropout and Strides for Larger Models](https://www.kaggle.com/dansbecker/dropout-and-strides-for-larger-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is Lesson 8 in the [Deep Learning](https://www.kaggle.com/learn/deep-learning) track.**  \n",
    "At the end of this lesson, you will understand and know how to use stride lengths to make your model faster and reduce memory consumption, as well as dropout to combat overfitting.  \n",
    "Both of these techniques are especially useful in large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDBoYFhsaGRoeHRsfIiUlIiIiIicnKiUnLigyMC0n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\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA//2Q==\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"450\"\n",
       "            src=\"https://www.youtube.com/embed/fwNLf4t7MR8\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x106c99198>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('fwNLf4t7MR8', width=800, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamingrove/.pyenv/versions/3.6.1/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep the data:\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "def data_prep(raw):\n",
    "    out_y = keras.utils.to_categorical(raw.label, num_classes)\n",
    "    \n",
    "    num_images = raw.shape[0]\n",
    "    x_as_array = raw.values[:,1:]\n",
    "    x_shaped_array = x_as_array.reshape(num_images, img_rows, img_cols, 1)\n",
    "    out_x = x_shaped_array / 255\n",
    "    return out_x, out_y\n",
    "\n",
    "train_size = 30000\n",
    "train_file = 'inputs/digit_recognizer/train.csv'\n",
    "raw_data = pd.read_csv(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model:\n",
    "\n",
    "x, y = data_prep(raw_data)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(30, kernel_size=(3,3),\n",
    "                strides=2,\n",
    "                activation='relu',\n",
    "                input_shape=(img_rows, img_cols, 1)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(30, kernel_size=(3,3), strides=2, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/2\n",
      "33600/33600 [==============================] - 5s 150us/step - loss: 0.6150 - acc: 0.8056 - val_loss: 0.2165 - val_acc: 0.9385\n",
      "Epoch 2/2\n",
      "33600/33600 [==============================] - 5s 145us/step - loss: 0.2703 - acc: 0.9173 - val_loss: 0.1401 - val_acc: 0.9589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x105788f28>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and fit the model:\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x, y,\n",
    "         batch_size=128,\n",
    "         epochs=2,\n",
    "         validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Exercise: Dropout and Strides For Larger Models](https://www.kaggle.com/dansbecker/exercise-dropout-and-strides-for-larger-models/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've built a model to identify clothing types in the MNIST for Fashion dataset.  \n",
    "Now you will make your model bigger, specify larger stride lengths and apply dropout.  \n",
    "These changes will make your model faster and more accurate.\n",
    "This is the last step in the Deep Learning Track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python import keras\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "def prep_data(raw, train_size, val_size):\n",
    "    y = raw[:, 0]\n",
    "    out_y = keras.utils.to_categorical(y, num_classes)\n",
    "    x = raw[:,1:]\n",
    "    num_images = raw.shape[0]\n",
    "    out_x = x.reshape(num_images, img_rows, img_cols, 1)\n",
    "    out_x = out_x / 255\n",
    "    return out_x, out_y\n",
    "\n",
    "fashion_file = 'inputs/fashionmnist/train.csv'\n",
    "fashion_data = np.loadtxt(fashion_file, skiprows=1, delimiter=',')\n",
    "x, y = prep_data(fashion_data, train_size=50000, val_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/3\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.6665 - acc: 0.7657 - val_loss: 0.4930 - val_acc: 0.8241\n",
      "Epoch 2/3\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.4475 - acc: 0.8385 - val_loss: 0.4275 - val_acc: 0.8498\n",
      "Epoch 3/3\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.3991 - acc: 0.8565 - val_loss: 0.3941 - val_acc: 0.8604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x12939f198>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_model = Sequential()\n",
    "fashion_model.add(Conv2D(12, kernel_size=(3, 3), strides=2, activation='relu',\n",
    "                        input_shape=(img_rows, img_cols, 1)))\n",
    "fashion_model.add(Conv2D(12, (3, 3), strides=2, activation='relu'))\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(128, activation='relu'))\n",
    "fashion_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "fashion_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                     optimizer='adam',\n",
    "                     metrics=['accuracy'])\n",
    "batch_size = 128\n",
    "epochs = 3\n",
    "fashion_model.fit(x, y,\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Strides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your turn!  \n",
    "Specify and fit a model much like the one above, but specify a stride length of 2 for each convolutional layer.  \n",
    "Call your new model `fashion_model_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/2\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.6743 - acc: 0.7617 - val_loss: 0.5064 - val_acc: 0.8188\n",
      "Epoch 2/2\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.4339 - acc: 0.8428 - val_loss: 0.4222 - val_acc: 0.8523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x12a4b1160>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout\n",
    "\n",
    "fashion_model_1 = Sequential()\n",
    "fashion_model_1.add(Conv2D(12, kernel_size=(3, 3), strides=2,\n",
    "                          activation='relu',\n",
    "                          input_shape=(img_rows, img_cols, 1)))\n",
    "fashion_model_1.add(Conv2D(12, (3, 3), strides=2, activation='relu'))\n",
    "fashion_model_1.add(Flatten())\n",
    "fashion_model_1.add(Dense(128, activation='relu'))\n",
    "fashion_model_1.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "fashion_model_1.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                        optimizer='adam',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "fashion_model_1.fit(x, y,\n",
    "                  batch_size=128,\n",
    "                  epochs=2,\n",
    "                  validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Model Larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
