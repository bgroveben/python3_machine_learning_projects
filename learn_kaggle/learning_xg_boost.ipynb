{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Learning to Use XGBoost](https://www.kaggle.com/dansbecker/learning-to-use-xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost** is the leading model for working with standard tabular data (the type of data you store in pandas DataFrames, as opposed to more exotic types of data like images and videos).  \n",
    "XGBoost models do well in many Kaggle competitions.  \n",
    "To reach peak accuracy, XGBoost models require more knowledge and model tuning than techniques like Random Forest. After this tutorial, you'll be able to:  \n",
    "* Follow the full modeling workflow with XGBoost, and    \n",
    "* Fine-tune XGBoost models for optimal performance\n",
    "\n",
    "XGBoost is an implementation of the Gradient Boosted Decision Trees algorithm (scikit-learn has another version of this algorithm, but XGBoost has some technical advantages.)  \n",
    "What are Gradient Boosted Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![XGBoost](img/xgboost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New models are generated in cycles, and the results of these models are aggregated and used to build into an **ensemble** model.  \n",
    "We start the cycle by calculating the errors for each observation in the dataset.  \n",
    "We then build a new model to predict those errors.  \n",
    "We add predictions from this error-predicting model to the ensemble of models.  \n",
    "To make a prediction, we include the predictions from all previous models.  \n",
    "We can use these predictions to calculate new errors, build the next model, and add it to the ensemble.  \n",
    "There's one piece outside that cycle.  \n",
    "We need some base prediction to start the cycle.  \n",
    "In practice, the initial predictions can be pretty naive.  \n",
    "Even if the predictions are wildly inaccurate, subsequent additions to the ensemble will address those errors.  \n",
    "This process may sound complicated, but the code to use it is straightforward.  \n",
    "We'll fill in some additional explanatory details in the model tuning section below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First entry of train_X :\n",
      " [[8.780e+02 6.000e+01 7.400e+01 8.834e+03 9.000e+00 5.000e+00 2.004e+03\n",
      "  2.005e+03 2.160e+02 1.170e+03 0.000e+00 2.920e+02 1.462e+03 1.462e+03\n",
      "  7.620e+02 0.000e+00 2.224e+03 1.000e+00 0.000e+00 2.000e+00 1.000e+00\n",
      "  4.000e+00 1.000e+00 1.000e+01 1.000e+00 2.004e+03 3.000e+00 7.380e+02\n",
      "  1.840e+02 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  6.000e+00 2.009e+03]]\n",
      "\n",
      "First entry of test_X :\n",
      " [[8.200e+02 1.200e+02 4.400e+01 6.371e+03 7.000e+00 5.000e+00 2.009e+03\n",
      "  2.010e+03 1.280e+02 7.330e+02 0.000e+00 6.250e+02 1.358e+03 1.358e+03\n",
      "  0.000e+00 0.000e+00 1.358e+03 1.000e+00 0.000e+00 2.000e+00 0.000e+00\n",
      "  2.000e+00 1.000e+00 6.000e+00 1.000e+00 2.010e+03 2.000e+00 4.840e+02\n",
      "  1.920e+02 3.500e+01 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  6.000e+00 2.010e+03]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "data = pd.read_csv('input/train.csv')\n",
    "data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = data.SalePrice\n",
    "X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\n",
    "train_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n",
    "my_imputer = Imputer()\n",
    "train_X = my_imputer.fit_transform(train_X)\n",
    "print(\"First entry of train_X :\\n\", train_X[:1])\n",
    "print()\n",
    "test_X = my_imputer.transform(test_X)\n",
    "print(\"First entry of test_X :\\n\", test_X[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build and fit a model just as we would in `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "my_model = XGBRegressor()\n",
    "# Add silent=True to avoid printing out updates with each cycle:\n",
    "# Don't forget to examine the parameters displayed when the model is built.\n",
    "# Tuning those parameters properly may improve the model's performance.\n",
    "my_model.fit(train_X, train_y, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now on to evaluating the model and making predictions, also like in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([212101.77, 181076.14, 116821.5 , 188081.03, 183677.23],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = my_model.predict(test_X)\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error:\n",
      " 16368.89558005137\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(\"Mean Absolute Error:\\n\", str(mean_absolute_error(predictions, test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
