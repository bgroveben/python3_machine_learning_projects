{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Handling Missing Values](https://www.kaggle.com/dansbecker/handling-missing-values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note about variable scope in Jupyter Notebooks (.ipynb files)  \n",
    "\n",
    "When a variable is declared in a cell and that cell is run, the variable enters the global namespace for that notebook.  \n",
    "If you want to restrict variables to a local scope, wrap them in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuz_i'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def foo():\n",
    "    y = 'cuz_i'\n",
    "    return y\n",
    "\n",
    "foo()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'said_so'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bar():\n",
    "    y = 'said_so'\n",
    "    return y\n",
    "\n",
    "bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you aren't careful about variable scope, confusion may ensue; use caution.  \n",
    "Now on to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The scoop on missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways data can end up with missing values.    \n",
    "* A 2 bedroom house doesn't have value for a third bedroom.\n",
    "* Someone being surveyed may choose not to share their income.\n",
    "\n",
    "Python libraries represent missing numbers as `nan` which is short for \"not a number\".  \n",
    "You can detect which cells have missing values, and then count how many there are in each column with the command:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most libraries (including scikit-learn) will give you an error if you try to build a model using data with missing values.  \n",
    "Let's figure out how to deal with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. You can drop columns with missing values:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_without_missing_values = original_data.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to drop the same columns from the DataFrames in both your training dataset and test dataset:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cols_with_missing_vals = [col for col in original_data.columns\n",
    "                                      if original_data[col].isnull().any()]\n",
    "reduced_original_data = original_data.drop(cols_with_missing, axis=1)\n",
    "reduced_test_data = test_data.drop(cols_with_missing, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method discards all information in the entire column, so it can be useful when most values in a column are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. You can impute missing values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation replaces the missing value with some number (the mean, for example), which usually gives more accurate models than dropping the column entirely."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "my_imputer = Imputer()\n",
    "data_with_imputed_values = my_imputer.fit_transform(original_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation can also be included in a scikit-learn Pipeline, which simplify model building, validation, and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. You can extend imputation to consider which values were originally missing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation is the standard approach, and it usually works well.  \n",
    "However, imputed values may by systematically above or below their actual values (which weren't collected in the dataset).  \n",
    "Or rows with missing values may be unique in some other way.  \n",
    "In that case, your model would make better predictions by considering which values were originally missing.  \n",
    "Here's how it might look:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Make a copy to keep the original data intact:\n",
    "new_data = original_data.copy()\n",
    "\n",
    "# Make new columns indicating what will be imputed:\n",
    "cols_with_missing = [col for col in new_data.columns\n",
    "                                 if new_data[c].isnull().any()]\n",
    "# Compare and contrast:\n",
    "for col in cols_with_missing:\n",
    "    new_data[col + ' was missing'] = new_data[col].isnull()\n",
    "    \n",
    "# Impute:\n",
    "my_imputer = Imputer()\n",
    "new_data = my_imputer.fit_transform(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach may or may not improve the results compared to simply imputing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example comparing the solutions using the Melbourne Housing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see an example predicting housing prices from the Melbourne Housing data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mb_data = pd.read_csv('input/melbourne_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mb_target = mb_data.Price\n",
    "mb_predictors = mb_data.drop(['Price'], axis=1)\n",
    "\n",
    "# In order to simplify this example, only numeric predictors are used.\n",
    "mb_numeric_predictors = mb_predictors.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to measure how well each approach performs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide our data into training and test.  \n",
    "We've loaded a function `score_dataset(X_train, X_test, y_train, y_test)` to compare the quality of different approaches to missing values.  \n",
    "This function reports the out-of-sample MAE score from a RandomForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(mb_numeric_predictors, mb_target, train_size=0.7,\n",
    "                                                   test_size=0.3, random_state=0)\n",
    "\n",
    "def score_dataset(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return mae(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping columns with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error after dropping columns with missing values:\n",
      "349491.40597934404\n"
     ]
    }
   ],
   "source": [
    "cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_test = X_test.drop(cols_with_missing, axis=1)\n",
    "print(\"Mean Absolute Error after dropping columns with missing values:\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model score from imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error after imputing misssing values:\n",
      "204969.41964123936\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "my_imputer = Imputer()\n",
    "imputed_X_train = my_imputer.fit_transform(X_train)\n",
    "imputed_X_test = my_imputer.transform(X_test)\n",
    "print(\"Mean Absolute Error after imputing misssing values:\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get score after imputation and display imputed values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error while tracking imputed values:\n",
      "203061.2769885849\n"
     ]
    }
   ],
   "source": [
    "imputed_X_train_plus = X_train.copy()\n",
    "imputed_X_test_plus = X_test.copy()\n",
    "\n",
    "# https://www.python.org/dev/peps/pep-0289/\n",
    "cols_with_missing = (col for col in X_train.columns if X_train[col].isnull().any())\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    imputed_X_train_plus[col + ' was missing'] = imputed_X_train_plus[col].isnull()\n",
    "    imputed_X_test_plus[col + ' was missing'] = imputed_X_test_plus[col].isnull()\n",
    "    \n",
    "# Imputation\n",
    "my_imputer = Imputer()\n",
    "imputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)\n",
    "imputed_X_test_plus = my_imputer.transform(imputed_X_test_plus)\n",
    "\n",
    "print(\"Mean Absolute Error while tracking imputed values:\")\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between imputation and imputation with extension is trivial compared to dropping entire columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Categorical Data and One-Hot Encoding](https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical data is data that takes only a limited number of values.  \n",
    "For example, if you people responded to a survey about which what brand of car they owned, the result would be categorical (because the answers would be things like Honda, Toyota, Ford, None).  \n",
    "Responses fall into a fixed set of categories.  \n",
    "You will get an error if you try to plug these variables into most machine learning models in Python without \"encoding\" them first.  \n",
    "Here we'll show the most popular method for encoding categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding is the most widespread approach, and it works very well unless your categorical variables take on a large number of values.   \n",
    "You generally won't use it for variables taking more than about a dozen different values.  \n",
    "One hot encoding creates new (binary) columns, indicating the presence of each possible value from the original data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![one_hot_encoding](img/one_hot_encoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in the original data are Red, Yellow, and Green.  \n",
    "We create a separate column for each possible value.  \n",
    "Wherever the original value was Red, a 1 is entered into the Red column, and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin at the point where the **train_predictors** and **test_predictors** DataFrames set up.  \n",
    "This data contains the housing characteristics.  \n",
    "You will use them to predict home prices, which are stored in a pandas Series called **target**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('input/train.csv')\n",
    "test_data = pd.read_csv('input/test.csv')\n",
    "\n",
    "# Disregard the houses that are missing the target value:\n",
    "train_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "\n",
    "target = train_data.SalePrice\n",
    "\n",
    "# Keep this simple (and inaccurate, remember) and just drop columns with missing values.\n",
    "# Try experimenting with different and better ways to go about this.\n",
    "cols_with_missing = [col for col in train_data.columns if train_data[col].isnull().any()]\n",
    "\n",
    "# Let's get some predictors chosen now:\n",
    "candidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\n",
    "candidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)\n",
    "\n",
    "# Cardinality == number of unique values in a column.\n",
    "# This is a convenient (and arbitrary) way of selecting categorical columns.\n",
    "\n",
    "low_cardinality_cols = [cname for cname in candidate_train_predictors.columns if \n",
    "                                candidate_train_predictors[cname].nunique() < 10 and\n",
    "                                candidate_train_predictors[cname].dtype == \"object\"]\n",
    "\n",
    "numeric_cols = [cname for cname in candidate_train_predictors.columns if \n",
    "                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "my_cols = low_cardinality_cols + numeric_cols\n",
    "train_predictors = candidate_train_predictors[my_cols]\n",
    "test_predictors = candidate_test_predictors[my_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas assigns a data type `dtype` to each Series (column).  \n",
    "Let's take a look at a random sample of dtypes from the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YearBuilt        int64\n",
       "BsmtHalfBath     int64\n",
       "MSSubClass       int64\n",
       "ScreenPorch      int64\n",
       "MSZoning        object\n",
       "GarageCars       int64\n",
       "TotRmsAbvGrd     int64\n",
       "CentralAir      object\n",
       "FullBath         int64\n",
       "OverallCond      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictors.dtypes.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Object** usually indicates a column has text.  \n",
    "It's common to one-hot encode these \"object\" columns, since they can't be plugged directly into most models.  \n",
    "With pandas you can use the `get_dummies()` function for one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>706</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>856</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>978</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>1262</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>434</td>\n",
       "      <td>920</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>756</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>655</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>1145</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 159 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotArea  OverallQual  OverallCond  YearBuilt  YearRemodAdd  \\\n",
       "0          60     8450            7            5       2003          2003   \n",
       "1          20     9600            6            8       1976          1976   \n",
       "2          60    11250            7            5       2001          2002   \n",
       "3          70     9550            7            5       1915          1970   \n",
       "4          60    14260            8            5       2000          2000   \n",
       "\n",
       "   BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF          ...            \\\n",
       "0         706           0        150          856          ...             \n",
       "1         978           0        284         1262          ...             \n",
       "2         486           0        434          920          ...             \n",
       "3         216           0        540          756          ...             \n",
       "4         655           0        490         1145          ...             \n",
       "\n",
       "   SaleType_ConLw  SaleType_New  SaleType_Oth  SaleType_WD  \\\n",
       "0               0             0             0            1   \n",
       "1               0             0             0            1   \n",
       "2               0             0             0            1   \n",
       "3               0             0             0            1   \n",
       "4               0             0             0            1   \n",
       "\n",
       "   SaleCondition_Abnorml  SaleCondition_AdjLand  SaleCondition_Alloca  \\\n",
       "0                      0                      0                     0   \n",
       "1                      0                      0                     0   \n",
       "2                      0                      0                     0   \n",
       "3                      1                      0                     0   \n",
       "4                      0                      0                     0   \n",
       "\n",
       "   SaleCondition_Family  SaleCondition_Normal  SaleCondition_Partial  \n",
       "0                     0                     1                      0  \n",
       "1                     0                     1                      0  \n",
       "2                     0                     1                      0  \n",
       "3                     0                     0                      0  \n",
       "4                     0                     1                      0  \n",
       "\n",
       "[5 rows x 159 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n",
    "one_hot_encoded_training_predictors[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you could have dropped the categoricals.  \n",
    "To see how the approaches compare, we can calculate the mean absolute error of models that are built with two alternative sets of predictors.  \n",
    "1. One-hot encoded cateoricals as well as numeric predictors.\n",
    "2. Numerical predictors, where we drop categoricals.  \n",
    "\n",
    "One-hot encoding usually helps, but it varies on a case-by-case basis.  \n",
    "In this case, there appears to be little meaningful benefit from using one-hot encoded variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18243.417545370303"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def get_mae(X, y):\n",
    "    # Convention is to return a positive MAE score, so multiply by -1 .\n",
    "    return -1 * cross_val_score(RandomForestRegressor(50), X, y, scoring = 'neg_mean_absolute_error').mean()\n",
    "\n",
    "predictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\n",
    "mae_without_categoricals = get_mae(predictors_without_categoricals, target)\n",
    "mae_without_categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17882.902608000048"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)\n",
    "mae_one_hot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying to Multiple Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, you've one-hot-encoded your training data.  \n",
    "What do you do when you have multiple files (e.g. a test dataset, or some other data that you'd like to make predictions for)?  \n",
    "Scikit-learn is sensitive to the ordering of columns, so if the training dataset and test datasets get misaligned, your results will be useless, maybe even misleading.  \n",
    "This could happen if a categorical had a different number of values in the training data vs the test data.  \n",
    "Ensure the test data is encoded in the same manner as the training data with the `align` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n",
    "one_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\n",
    "final_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors, \n",
    "                                                                    join='left', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>706</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>856</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>978</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>1262</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>434</td>\n",
       "      <td>920</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>756</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>655</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>1145</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 159 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotArea  OverallQual  OverallCond  YearBuilt  YearRemodAdd  \\\n",
       "0          60     8450            7            5       2003          2003   \n",
       "1          20     9600            6            8       1976          1976   \n",
       "2          60    11250            7            5       2001          2002   \n",
       "3          70     9550            7            5       1915          1970   \n",
       "4          60    14260            8            5       2000          2000   \n",
       "\n",
       "   BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF          ...            \\\n",
       "0         706           0        150          856          ...             \n",
       "1         978           0        284         1262          ...             \n",
       "2         486           0        434          920          ...             \n",
       "3         216           0        540          756          ...             \n",
       "4         655           0        490         1145          ...             \n",
       "\n",
       "   SaleType_ConLw  SaleType_New  SaleType_Oth  SaleType_WD  \\\n",
       "0               0             0             0            1   \n",
       "1               0             0             0            1   \n",
       "2               0             0             0            1   \n",
       "3               0             0             0            1   \n",
       "4               0             0             0            1   \n",
       "\n",
       "   SaleCondition_Abnorml  SaleCondition_AdjLand  SaleCondition_Alloca  \\\n",
       "0                      0                      0                     0   \n",
       "1                      0                      0                     0   \n",
       "2                      0                      0                     0   \n",
       "3                      1                      0                     0   \n",
       "4                      0                      0                     0   \n",
       "\n",
       "   SaleCondition_Family  SaleCondition_Normal  SaleCondition_Partial  \n",
       "0                     0                     1                      0  \n",
       "1                     0                     1                      0  \n",
       "2                     0                     1                      0  \n",
       "3                     0                     0                      0  \n",
       "4                     0                     1                      0  \n",
       "\n",
       "[5 rows x 159 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>11622</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>468.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>882.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>14267</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1958</td>\n",
       "      <td>1958</td>\n",
       "      <td>923.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>406.0</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>13830</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1997</td>\n",
       "      <td>1998</td>\n",
       "      <td>791.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>928.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>9978</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>926.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>5005</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 159 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotArea  OverallQual  OverallCond  YearBuilt  YearRemodAdd  \\\n",
       "0          20    11622            5            6       1961          1961   \n",
       "1          20    14267            6            6       1958          1958   \n",
       "2          60    13830            5            5       1997          1998   \n",
       "3          60     9978            6            6       1998          1998   \n",
       "4         120     5005            8            5       1992          1992   \n",
       "\n",
       "   BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF          ...            \\\n",
       "0       468.0       144.0      270.0        882.0          ...             \n",
       "1       923.0         0.0      406.0       1329.0          ...             \n",
       "2       791.0         0.0      137.0        928.0          ...             \n",
       "3       602.0         0.0      324.0        926.0          ...             \n",
       "4       263.0         0.0     1017.0       1280.0          ...             \n",
       "\n",
       "   SaleType_ConLw  SaleType_New  SaleType_Oth  SaleType_WD  \\\n",
       "0               0             0             0            1   \n",
       "1               0             0             0            1   \n",
       "2               0             0             0            1   \n",
       "3               0             0             0            1   \n",
       "4               0             0             0            1   \n",
       "\n",
       "   SaleCondition_Abnorml  SaleCondition_AdjLand  SaleCondition_Alloca  \\\n",
       "0                      0                      0                     0   \n",
       "1                      0                      0                     0   \n",
       "2                      0                      0                     0   \n",
       "3                      0                      0                     0   \n",
       "4                      0                      0                     0   \n",
       "\n",
       "   SaleCondition_Family  SaleCondition_Normal  SaleCondition_Partial  \n",
       "0                     0                     1                      0  \n",
       "1                     0                     1                      0  \n",
       "2                     0                     1                      0  \n",
       "3                     0                     1                      0  \n",
       "4                     0                     1                      0  \n",
       "\n",
       "[5 rows x 159 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The align command makes sure the columns show up in the same order in both datasets (it uses column names to identify which columns line up in each dataset.)  \n",
    "The argument `join='left'` specifies that we will do the equivalent of SQL's left join.  \n",
    "That means, if there are ever columns that show up in one dataset and not the other, we will keep the columns from our training data.  \n",
    "The argument `join='inner'` would do what SQL databases call an inner join, keeping only the columns showing up in both datasets.  \n",
    "That can also a be sensible choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The world is filled with categorical data.  \n",
    "You will be a much more effective data scientist if you know how to use this data.  \n",
    "Here are resources that will be useful as you start doing more sophisticated work with categorical data.  \n",
    "* **Pipelines**: Deploying models into production ready systems is a topic unto itself. While one-hot encoding is still a great approach, your code will need to built in an especially robust way. Scikit-learn pipelines are a great tool for this. Scikit-learn offers a class for one-hot encoding and this can be added to a Pipeline. Unfortunately, it doesn't handle text or object values, which is a common use case.  \n",
    "* **Applications To Text for Deep Learning**: Keras and TensorFlow have fuctionality for one-hot encoding, which is useful for working with text.  \n",
    "* **Categoricals with Many Values**: Scikit-learn's `FeatureHasher` uses the hashing trick to store high-dimensional data. This will add some complexity to your modeling code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
