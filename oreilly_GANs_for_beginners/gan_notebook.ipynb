{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks for Beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an artificial neural network that learns to generate handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By [Jon Bruner](https://github.com/jonbruner) and [Adit Deshpande](https://github.com/adeshpande3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create a GAN that will generate handwritten digits that can fool even the best classifiers (and humans as well).  \n",
    "We'll use [TensorFlow](https://www.tensorflow.org/), a deep learing library open-sourced by Google that makes it easy to train neural networks on GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a set of real handwritten digits to give the discriminator a starting point in distinguishing between real and fake images.  \n",
    "In this tutorial, we are going to use [MNIST](http://yann.lecun.com/exdb/mnist/), a benchmark dataset in deep learning.  \n",
    "Let us begin by importing TensorFlow along with a couple of other helpful libraries.  \n",
    "We'll also import our MNIST images using a TensorFlow convenience function called `read_data_sets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST variable we created above contains both the images and their labels, divided into a training set called `train` and a validation set called `validation`.  \n",
    "We can retrieve batches of images by calling `next_batch` on `mnist`.  \n",
    "Let's load up an image and have a look, shall we?  \n",
    "The images are initially formatted as a single row of 784 pixels.  \n",
    "We can reshape them into 28 x 28-pixel images and view them using pyplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1224f47f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e8b28d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_image = mnist.train.next_batch(1)[0]\n",
    "print(sample_image.shape)\n",
    "\n",
    "sample_image = sample_image.reshape([28, 28])\n",
    "plt.imshow(sample_image, cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the cell above again, you'll see a different image form the MNIST training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our discriminator is a convolutional neural network that takes in an image of size 28 x 28 x 1 as input and returns a single scalar number that describes whether or not the input image is \"real\" or \"fake\" -- that is, whether it's drawn from the set of MNIST images or generated by the generator.  \n",
    "The structure of the discriminator network features two convolutional layers that find 5 x 5 pixel features, and two \"fully connected\" layers that multiply weights by every pixel in the image.  \n",
    "To set up each layer, we start by creating weight and bias variables through `tf.get_variable`.  \n",
    "Weights are initialized from a [truncated normal distribution](https://www.tensorflow.org/api_docs/python/tf/truncated_normal), and biases are initialized at zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.nn.conv2d()` is TensorFlow's standard convolution function.  \n",
    "It takes 4 arguments.  \n",
    "The first is the input volume (our `28 x 28 x 1` images in this case).  \n",
    "The next argument is the filter/weight matrix.  \n",
    "Finally, you can also change the stride and padding of the convolution.  \n",
    "those two values affect the dimensions of the output volume.  \n",
    "If you're already comfortable with CNNs, you'll recognize this as a simple binary classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(images, reuse=False):\n",
    "    if (reuse):\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        \n",
    "    # First convolutional and pool layers.\n",
    "    # This finds 32 different 5 x 5 pixel features:\n",
    "    d_w1 = tf.get_variable('d_w1', [5, 5, 1, 32],\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    d_b1 = tf.get_variable('d_b1', [32],\n",
    "                          initializer=tf.constant_initializer(0))\n",
    "    d1 = tf.nn.conv2d(input=images, filter=d_w1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    d1 = d1 + d_b1\n",
    "    d1 = tf.nn.relu(d1)\n",
    "    d1 = tf.nn.avg_pool(d1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    # Second convolutional and pool layers.\n",
    "    # This finds 64 different 5 x 5 pixel features:\n",
    "    d_w2 = tf.get_variable('d_w2', [5, 5, 32, 64],\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    d_b2 = tf.get_variable('d_b2', [64],\n",
    "                          initializer=tf.constant_initializer(0))\n",
    "    d2 = tf.nn.conv2d(input=d1, filter=d_w2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    d2 = d2 + d_b2\n",
    "    d2 = tf.nn.relu(d2)\n",
    "    d2 = tf.nn.avg_pool(d2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    # First fully connected layer:\n",
    "    d_w3 = tf.get_variable('d_w3', [7 * 7 * 64, 1024],\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    d_b3 = tf.get_variable('d_b3', [1024],\n",
    "                          initializer=tf.constant_initializer(0))\n",
    "    d3 = tf.reshape(d2, [-1, 7 * 7 * 64])\n",
    "    d3 = tf.matmul(d3, d_w3)\n",
    "    d3 = d3 + d_b3\n",
    "    d3 = tf.nn.relu(d3)\n",
    "    \n",
    "    # Second fully connected layer:\n",
    "    d_w4 = tf.get_variable('d_w4', [1024, 1],\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    d_b4 = tf.get_variable('d_b4', [1],\n",
    "                          initializer=tf.constant_initializer(0))\n",
    "    d4 = tf.matmul(d3, d_w4) + d_b4\n",
    "    \n",
    "    # d4 contains unscaled values:\n",
    "    return d4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our discriminator defined, let's take a look at the generator model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
