{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [How to Prepare Text Data for Machine Learning with scikit-learn](https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/)  \n",
    "\n",
    "## by Jason Brownlee on September 29, 2017 in Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data requires special preparation before you can start using it for predictive modeling.  \n",
    "The text must be parsed to remove words, called tokenization.  \n",
    "Then the words need to be encoded as integers or floating point values for use as input to a machine learning algorithm, called feature extraction (or vectorization).  \n",
    "The scikit-learn library offers easy-to-use tools to perform both tokenization and feature extraction of your text data.  \n",
    "In this tutorial, you will discover exactly how you can prepare your text data for predictive modeling in Python with scikit-learn.  \n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "- How to convert text to word count vectors with CountVectorizer.\n",
    "- How to convert text to word frequency vectors with TfidfVectorizer.\n",
    "- How to convert text to unique integers with HashingVectorizer.  \n",
    "\n",
    "Let’s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot work with text directly when using machine learning algorithms.  \n",
    "Instead, we need to convert the text to numbers.  \n",
    "We may want to perform classification of documents, so each document is an “input” and a class label is the “output” for our predictive algorithm.  \n",
    "Algorithms take vectors of numbers as input, therefore we need to convert documents to fixed-length vectors of numbers.  \n",
    "A simple and effective model for thinking about text documents in machine learning is called the Bag-of-Words Model, or BoW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is simple in that it throws away all of the order information in the words and focuses on the occurrence of words in a document.  \n",
    "This can be done by assigning each word a unique number.  \n",
    "Then any document we see can be encoded as a fixed-length vector with the length of the vocabulary of known words.  \n",
    "The value in each position in the vector could be filled with a count or frequency of each word in the encoded document.  \n",
    "This is the bag of words model, where we are only concerned with encoding schemes that represent what words are present or the degree to which they are present in encoded documents without any information about order.  \n",
    "There are many ways to extend this simple method, both by better clarifying what a “word” is and in defining what to encode about each word in the vector.  \n",
    "The scikit-learn library provides 3 different schemes that we can use, and we will briefly look at each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Counts with CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.  \n",
    "You can use it as follows:\n",
    "1. Create an instance of the CountVectorizer class.\n",
    "2. Call the fit() function in order to learn a vocabulary from one or more documents.\n",
    "3. Call the transform() function on one or more documents as needed to encode each as a vector.  \n",
    "\n",
    "An encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document.  \n",
    "Because these vectors will contain a lot of zeros, we call them sparse.  \n",
    "Python provides an efficient way of handling sparse vectors in the scipy.sparse package.  \n",
    "The vectors returned from a call to transform() will be sparse vectors, and you can transform them back to numpy arrays to look and better understand what is going on by calling the toarray() function.  \n",
    "Below is an example of using the CountVectorizer to tokenize, build a vocabulary, and then encode a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer.vocabulary: {'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "vector.shape: (1, 8)\n",
      "type(vector): <class 'scipy.sparse.csr.csr_matrix'>\n",
      "vector.toarray(): [[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a list of text documents:\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# Create the transform:\n",
    "vectorizer = CountVectorizer()\n",
    "# Tokenize and build vocabulary:\n",
    "vectorizer.fit(text)\n",
    "# Summarize:\n",
    "print(\"vectorizer.vocabulary: {}\".format(vectorizer.vocabulary_))\n",
    "# Encode the document:\n",
    "vector = vectorizer.transform(text)\n",
    "# Summarize the encoded vector:\n",
    "print(\"vector.shape: {}\".format(vector.shape))\n",
    "print(\"type(vector): {}\".format(type(vector)))\n",
    "print(\"vector.toarray(): {}\".format(vector.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you can see that we access the vocabulary to see what exactly was tokenized by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all words were made lowercase by default and that the punctuation was ignored.  \n",
    "These and other aspects of tokenizing can be configured and I encourage you to review all of the options in the API documentation.  \n",
    "Running the example first prints the vocabulary, then the shape of the encoded document.  \n",
    "We can see that there are 8 words in the vocab, and therefore encoded vectors have a length of 8.  \n",
    "We can then see that the encoded vector is a sparse matrix.  \n",
    "Finally, we can see an array version of the encoded vector showing a count of 1 occurrence for each word except the (index and id 7) that has an occurrence of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector.shape: (1, 8)\n",
      "type(vector): <class 'scipy.sparse.csr.csr_matrix'>\n",
      "vector.toarray(): [[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"vector.shape: {}\".format(vector.shape))\n",
    "print(\"type(vector): {}\".format(type(vector)))\n",
    "print(\"vector.toarray(): {}\".format(vector.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, the same vectorizer can be used on documents that contain words not included in the vocabulary.  \n",
    "These words are ignored and no count is given in the resulting vector.  \n",
    "For example, below is an example of using the vectorizer above to encode a document with one word in the vocab and one word that is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Encode another sample document:\n",
    "text2 = [\"the puppy\"]\n",
    "vector = vectorizer.transform(text2)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this example prints the array version of the encoded sparse vector showing one occurrence of the one word in the vocabulary and the other word in the vocabulary ignored completely.  \n",
    "The encoded vectors can then be used directly with a machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
